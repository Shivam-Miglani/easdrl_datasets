{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrhl++GKE+G2Mg/yxKMEYM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivam-Miglani/easdrl_datasets/blob/main/nltopddl_data_emb_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flair datasets"
      ],
      "metadata": {
        "id": "Qb7hqSm17UaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "o-CEyDOn6auf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from flair.data import Sentence\n",
        "from flair.embeddings import WordEmbeddings, StackedEmbeddings, TransformerWordEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = \"win2k\""
      ],
      "metadata": {
        "id": "ghR4aC7dh-TD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACT"
      ],
      "metadata": {
        "id": "7g01qB1IPBQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_names = [\"glove\", \"en\", \"bert-base-uncased\",\"roberta-base\",  \"xlm-roberta-base\"]\n",
        "\n",
        "def assign_rl_actions(x):\n",
        "    rl_acts = np.ones(len(x[\"tokens\"]), dtype=np.int8)\n",
        "    for acts in x[\"acts\"]:\n",
        "        rl_acts[acts[\"act_idx\"]] = acts[\"act_type\"] + 1\n",
        "    return rl_acts\n",
        "\n",
        "def get_stacked_embedding(emb_name, mean=True):\n",
        "    \"\"\"\n",
        "    Use flair to get stacked transformer embeddings or just glove embedding.\n",
        "    Following emb_name are possible:\n",
        "    - glove\n",
        "    - bert-base-uncased\n",
        "    - roberta-base\n",
        "\n",
        "    With mean = True, emb_size: 768 (bert-base) + 100 (glove) = 868\n",
        "    With mean = False, emb_size: 768 (bert-base) * 4 (layers) + 100 (glove) = 3172\n",
        "    \"\"\"\n",
        "    if emb_name == \"glove\":\n",
        "        return WordEmbeddings(\"glove\")\n",
        "    elif emb_name == \"en\":\n",
        "        return WordEmbeddings(\"en\")\n",
        "    else:\n",
        "        return StackedEmbeddings(\n",
        "            [\n",
        "                WordEmbeddings(\"glove\"),\n",
        "                TransformerWordEmbeddings(emb_name, layers=\"-1,-2,-3,-4\", layer_mean=mean, \n",
        "                                          subtoken_pooling=\"mean\", use_context=True),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "def add_padding(x):\n",
        "    pad_len = NUM_WORDS - len(x[\"tokens\"])\n",
        "    if pad_len > 0:\n",
        "      x[\"rl_actions\"] = np.concatenate((x[\"rl_actions\"], np.ones(pad_len, dtype=np.int32))).reshape(\n",
        "          NUM_WORDS, 1\n",
        "      )\n",
        "      x[\"doc_embs\"] = np.concatenate((x[\"doc_embs\"], np.zeros([pad_len, WORD_DIM])))\n",
        "    else:\n",
        "      x[\"rl_actions\"] = x[\"rl_actions\"][: NUM_WORDS]\n",
        "      x[\"doc_embs\"] = x[\"doc_embs\"][: NUM_WORDS]\n",
        "    return x\n",
        "\n",
        "def get_doc_embeddings(sentences):\n",
        "    \"\"\"\n",
        "    generate stacked embedding from a sentence\n",
        "    \"\"\"\n",
        "    sent_vec = []\n",
        "    for sent in sentences:\n",
        "        sent = Sentence(sent, use_tokenizer=False)\n",
        "        STACKED_EMBEDDING.embed(sent)\n",
        "        for token in sent:\n",
        "            sent_vec.append(token.embedding.cpu().numpy())\n",
        "    return np.array(sent_vec)\n",
        "\n",
        "for i in range(len(emb_names)):\n",
        "  EMB_NAME = emb_names[i]\n",
        "  print(EMB_NAME)\n",
        "  act_df = pd.DataFrame(pd.read_pickle(f'{DATASET}_act.pkl'))\n",
        "  act_df = act_df.rename(columns={\"words\": \"tokens\"})\n",
        "  act_df[\"doc_len\"] = act_df.tokens.apply(lambda x: len(x))\n",
        "  act_df[\"exclusive_related_actions\"] = act_df.acts.apply(lambda x: {y[\"act_idx\"]: y[\"related_acts\"] for y in x})\n",
        "  act_df[\"rl_actions\"] = act_df.apply(assign_rl_actions, axis=1)\n",
        "  act_df[\"sents_\"] = act_df.sents.apply(lambda x: [\" \".join(y) for y in x])\n",
        "\n",
        "  \n",
        "  STACKED_EMBEDDING = get_stacked_embedding(EMB_NAME)\n",
        "\n",
        "  act_df[\"doc_embs\"] = act_df.sents_.apply(get_doc_embeddings)\n",
        "\n",
        "  NUM_WORDS = 512\n",
        "  WORD_DIM = act_df.doc_embs.loc[0][0].shape[0]\n",
        "\n",
        "  act_df = act_df.apply(add_padding, axis=1)\n",
        "  del act_df[\"sents_\"]\n",
        "  del act_df[\"sent_acts\"]\n",
        "  del act_df[\"word2sent\"]\n",
        "  del act_df[\"acts\"]\n",
        "  del act_df[\"doc_len\"]\n",
        "  act_df['state_with_label'] = act_df.apply(lambda x: np.concatenate((x['doc_embs'], x['rl_actions'].reshape(NUM_WORDS, 1)), axis=1), axis=1)\n",
        "  del act_df['sents']\n",
        "  del act_df['doc_embs']\n",
        "  del act_df['rl_actions']\n",
        "\n",
        "  act_df.to_csv(f\"{DATASET}_act_{EMB_NAME}.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHTQ_ud98qJt",
        "outputId": "54a2f737-09d8-4a8e-ae37-aafd9bd0d9d2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "glove\n",
            "en\n",
            "bert-base-uncased\n",
            "roberta-base\n",
            "xlm-roberta-base\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ARG"
      ],
      "metadata": {
        "id": "pcRsy2YOMk8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "NUM_WORDS=150\n",
        "emb_names = [\"glove\", \"en\", \"bert-base-uncased\",\"roberta-base\",  \"xlm-roberta-base\"]\n",
        "\n",
        "def get_stacked_embedding(emb_name, mean=True):\n",
        "    \"\"\"\n",
        "    Use flair to get stacked transformer embeddings or just glove embedding.\n",
        "    Following emb_name are possible:\n",
        "    - glove\n",
        "    - bert-base-uncased\n",
        "    - roberta-base\n",
        "\n",
        "    With mean = True, emb_size: 768 (bert-base) + 100 (glove) = 868\n",
        "    With mean = False, emb_size: 768 (bert-base) * 4 (layers) + 100 (glove) = 3172\n",
        "    \"\"\"\n",
        "    if emb_name == \"glove\":\n",
        "        return WordEmbeddings(\"glove\")\n",
        "    elif emb_name == \"en\":\n",
        "        return WordEmbeddings(\"en\")\n",
        "    else:\n",
        "        return StackedEmbeddings(\n",
        "            [\n",
        "                WordEmbeddings(\"glove\"),\n",
        "                TransformerWordEmbeddings(emb_name, layers=\"-1,-2,-3,-4\", layer_mean=mean, \n",
        "                                          subtoken_pooling=\"mean\", use_context=True),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "def gen_stacked_embedding(words):\n",
        "    \"\"\"\n",
        "    generate stacked embedding from a list of words\n",
        "    \"\"\"\n",
        "    sent_vec = []\n",
        "    # Stacked embeddings\n",
        "    line = \" \".join(words)\n",
        "    sent = Sentence(line, use_tokenizer=False)\n",
        "    STACKED_EMBEDDING.embed(sent)\n",
        "    for token in sent:\n",
        "        sent_vec.append(token.embedding.cpu().numpy())\n",
        "    return np.array(sent_vec)\n",
        "\n",
        "for i in range(0, len(emb_names)):\n",
        "  EMB_NAME = emb_names[i]\n",
        "  STACKED_EMBEDDING = get_stacked_embedding(EMB_NAME)\n",
        "  docs = pd.read_pickle(f\"{DATASET}_arg.pkl\")[-1]\n",
        "  arg_sents = []\n",
        "  for i in tqdm(range(len(docs)), position=0, leave=True):\n",
        "      for j in range(len(docs[i])):\n",
        "          if len(docs[i][j]) == 0:\n",
        "              continue\n",
        "          words = docs[i][j][\"last_sent\"] + docs[i][j][\"this_sent\"]\n",
        "          sent_len = len(words)  # here sent len is last_sent + this_sent\n",
        "          act_inds = [\n",
        "              a[\"act_idx\"] for a in docs[i][j][\"acts\"] if a[\"act_idx\"] < NUM_WORDS\n",
        "          ]\n",
        "\n",
        "          # for each ground-truth action in each doc's each sample\n",
        "          for k in range(len(docs[i][j][\"acts\"])):\n",
        "              act_ind = docs[i][j][\"acts\"][k][\"act_idx\"]  # action index\n",
        "              # object index list\n",
        "              obj_inds = docs[i][j][\"acts\"][k][\"obj_idxs\"]\n",
        "              arg_sent = {}\n",
        "\n",
        "              # assign rl_action tags\n",
        "              arg_tags = np.ones(sent_len, dtype=np.int32)  # tags\n",
        "              if len(obj_inds[1]) == 0:\n",
        "                  arg_tags[obj_inds[0]] = 2  # essential objects\n",
        "              else:\n",
        "                  arg_tags[obj_inds[0]] = 4  # exclusive objects\n",
        "                  arg_tags[obj_inds[1]] = 4  # exclusive objects\n",
        "\n",
        "              # gen. distance repr\n",
        "              position = np.zeros(sent_len, dtype=np.int32)\n",
        "              position.fill(act_ind)\n",
        "              distance = np.abs(np.arange(sent_len) - position)\n",
        "\n",
        "              arg_sent[\"tokens\"] = words\n",
        "              arg_sent[\"tags\"] = arg_tags\n",
        "              arg_sent[\"act_ind\"] = act_ind\n",
        "              arg_sent[\"distance\"] = distance\n",
        "              arg_sent[\"act_inds\"] = act_inds\n",
        "              arg_sent[\"obj_inds\"] = obj_inds\n",
        "\n",
        "              # Stacked embeddings\n",
        "              sent_vec = gen_stacked_embedding(words)\n",
        "              WORD_DIM = sent_vec[0].shape[0]\n",
        "\n",
        "              # Padding\n",
        "              pad_len = NUM_WORDS - len(sent_vec)\n",
        "              distance = np.zeros([NUM_WORDS, 1])\n",
        "\n",
        "              if pad_len > 0:\n",
        "                  sent_vec = np.concatenate((sent_vec, np.zeros([pad_len, WORD_DIM])))\n",
        "                  arg_sent[\"tags\"] = np.concatenate(\n",
        "                      (arg_sent[\"tags\"], np.ones(pad_len, dtype=np.int32))\n",
        "                  )\n",
        "                  for d in range(len(arg_sent[\"distance\"])):\n",
        "                      distance[d] = arg_sent[\"distance\"][d]\n",
        "              else:\n",
        "                  sent_vec = sent_vec[: NUM_WORDS]\n",
        "                  arg_sent[\"tokens\"] = arg_sent[\"tokens\"][: NUM_WORDS]\n",
        "                  arg_sent[\"tags\"] = np.array(arg_sent[\"tags\"])[: NUM_WORDS]\n",
        "                  for d in range(NUM_WORDS):\n",
        "                      distance[d] = arg_sent[\"distance\"][d]\n",
        "\n",
        "              # RL State for arg DQN\n",
        "              sent_vec = np.concatenate((sent_vec, distance), axis=1)\n",
        "              arg_sent[\"sent_vec\"] = sent_vec\n",
        "              arg_sent[\"tags\"].shape = (NUM_WORDS, 1)\n",
        "              arg_sents.append(arg_sent)\n",
        "  arg_df = pd.DataFrame(arg_sents)\n",
        "  arg_df['state_with_label'] = arg_df.apply(lambda x: np.concatenate((x['sent_vec'], x['tags']), axis=1), axis=1)\n",
        "  del arg_df['tags']\n",
        "  del arg_df['sent_vec']\n",
        "  del arg_df['distance']\n",
        "\n",
        "  arg_df.to_csv(f\"{DATASET}_arg_{EMB_NAME}.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hwhjs7r-9JaV",
        "outputId": "fa9f0acf-f3d3-4468-b69d-876ce4a1eea0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 154/154 [00:02<00:00, 75.75it/s]\n",
            "100%|██████████| 154/154 [00:02<00:00, 72.74it/s]\n",
            "100%|██████████| 154/154 [00:22<00:00,  6.81it/s]\n",
            "100%|██████████| 154/154 [00:22<00:00,  6.74it/s]\n",
            "100%|██████████| 154/154 [00:22<00:00,  6.86it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aIzpFBkLdWZ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}